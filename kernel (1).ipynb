{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pandas import datetime\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Bidirectional, Conv2D, MaxPooling2D, Lambda, MaxPool2D, BatchNormalization, Input, concatenate, K, Reshape, LSTM, CuDNNLSTM\nfrom keras.utils import np_utils\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import *\nfrom keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\nfrom keras.utils.np_utils import to_categorical\nfrom keras.losses import *\nfrom sklearn.preprocessing import LabelEncoder, minmax_scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nimport xml.etree.ElementTree as ET\nimport sklearn\nimport itertools\nimport cv2\nimport scipy\nimport os\nimport csv\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n%matplotlib inline\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "42966ee61574761bc91ea17d9af7364e190e6130"
      },
      "cell_type": "code",
      "source": "#Objective : Calculate sales/hour for a given system of system and manager scheduled hours\n\"\"\"\"The solution should also identify the impact of any\nchanges made by the manager on the system\ngenerated schedule and determine which\nmanagers make better changes to the system\ngenerated schedule or more generally, what\nfactors result in better schedule adjustments by\nmanagers vs changes that result in worse / non-\noptimal labor schedule & costs.\nObjectives : Maximise sales hour\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "14ff9d3f58570d36c93af07a1ad3253e2e062d72"
      },
      "cell_type": "code",
      "source": "df = pd.read_csv('../input/BITS AIC 2019 - Reflexis Raw Dataset.csv')\ndf['DATE'] = pd.to_datetime(df['DATE'])\ndf = df.set_index('DATE')\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "497424e94de85dfb74eab0d1baaec2dee88cc9c7"
      },
      "cell_type": "code",
      "source": "df['SALES_NORMALIZED'] = minmax_scale(df['SALES_ACTUAL'])\nprint('Max sales = ', df['SALES_ACTUAL'].max())\nprint('Number of records =', len(df))\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3c13bb956d7d2ab56ac7bf3da972b3a3852845bd"
      },
      "cell_type": "code",
      "source": "df = df.groupby('STORE')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1229d2337c9510c2454cdc9b26ac6705785510d2"
      },
      "cell_type": "code",
      "source": "for store_no, store_data in df:\n    print(store_no)\n    print(df.get_group(store_no).head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "865fc12b612773e22d7b485be0836610de2e578b"
      },
      "cell_type": "code",
      "source": "class MetricsCheckpoint(Callback):\n    \"\"\"Callback that saves metrics after each epoch\"\"\"\n    def __init__(self, savepath):\n        super(MetricsCheckpoint, self).__init__()\n        self.savepath = savepath\n        self.history = {}\n    def on_epoch_end(self, epoch, logs=None):\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        np.save(self.savepath, self.history)\n        \nclass MetricsCheckpoint(Callback):\n    \"\"\"Callback that saves metrics after each epoch\"\"\"\n    def __init__(self, savepath):\n        super(MetricsCheckpoint, self).__init__()\n        self.savepath = savepath\n        self.history = {}\n    def on_epoch_end(self, epoch, logs=None):\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        np.save(self.savepath, self.history)\n\ndef plotKerasLearningCurve():\n    plt.figure(figsize=(10,5))\n    metrics = np.load('logs.npy')[()]\n    filt = ['acc'] # try to add 'loss' to see the loss learning curve\n    for k in filter(lambda x : np.any([kk in x for kk in filt]), metrics.keys()):\n        l = np.array(metrics[k])\n        plt.plot(l, c= 'r' if 'val' not in k else 'b', label='val' if 'val' in k else 'train')\n        x = np.argmin(l) if 'loss' in k else np.argmax(l)\n        y = l[x]\n        plt.scatter(x,y, lw=0, alpha=0.25, s=100, c='r' if 'val' not in k else 'b')\n        plt.text(x, y, '{} = {:.4f}'.format(x,y), size='15', color= 'r' if 'val' not in k else 'b')   \n    plt.legend(loc=4)\n    plt.axis([0, None, None, None]);\n    plt.grid()\n    plt.xlabel('Number of epochs')\n    plt.ylabel('Accuracy')\n    \ndef plot_learning_curve(history):\n    plt.figure(figsize=(8,8))\n    plt.subplot(1,2,1)\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.savefig('./accuracy_curve.png')\n    #plt.clf()\n    # summarize history for loss\n    plt.subplot(1,2,2)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.savefig('./loss_curve.png')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "def f1(dfmain, store_no, epochs):\n    #predict sales given date, store number, system scheduled hours and manager scheduled hours\n    data = dfmain.get_group(store_no).copy()\n    data['SALES_DELTA'] = data['SALES_ACTUAL'].apply(lambda x: x- data['SALES_ACTUAL'].mean())\n    data['SALES_DELTA_NORM'] = minmax_scale(data['SALES_DELTA'], feature_range=(-1, 1))\n    batch_size = 16\n    split_date = '2018-01-01'\n    X_train, Y_train, X_test, Y_test = data[data.index < split_date][['MANAGER_SCHED_HOURS','SYSTEM_SCHED_HOURS']], data[data.index < split_date]['SALES_DELTA'], data[data.index > split_date][['MANAGER_SCHED_HOURS','SYSTEM_SCHED_HOURS']], data[data.index > split_date]['SALES_DELTA']\n    #X_train, X_test, Y_train, Y_test = train_test_split(data[['STORE','MANAGER_SCHED_HOURS','SYSTEM_SCHED_HOURS']],data['SALES_ACTUAL'], shuffle=False)\n    X_train = X_train.values.reshape(X_train.shape[0],1,X_train.shape[1])\n    X_test = X_test.values.reshape(X_test.shape[0],1,X_test.shape[1])\n    input_shape = (1,2)\n    num_classes = 1\n    model = Sequential()\n    model.add(Bidirectional(CuDNNLSTM(64, return_sequences=True, input_shape=input_shape)))\n    model.add(Dropout(0.25))\n    model.add(Bidirectional(CuDNNLSTM(64, return_sequences=True)))\n    model.add(Dropout(0.25))\n    model.add(Bidirectional(CuDNNLSTM(64)))\n    model.add(Dropout(0.25))\n    model.add(Dense(num_classes, activation='relu'))\n    model.compile(loss=mean_squared_error, optimizer = Adam(lr=0.01), metrics=['accuracy'])\n    history = model.fit(X_train, Y_train.values, epochs = epochs, steps_per_epoch=int(len(X_train) / batch_size), validation_data=(X_test, Y_test.values), validation_steps=max(int(len(X_test)/batch_size),1), callbacks = [MetricsCheckpoint('logs')])\n    score = model.evaluate(X_test, Y_test.values)\n    plotKerasLearningCurve()\n    plt.show()  \n    plot_learning_curve(history)\n    plt.show()\n    return model\n    \n#def f2(data):\n    #maximize sales, by adjusting sys_hrs and mgr_hrs for f1, output (sys_hrs,mgr_hrs)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "1b7a114960d9a1e012c9fc7411f10ef6d60a9c6a"
      },
      "cell_type": "code",
      "source": "lstm_model = f1(df, 203, 300)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}